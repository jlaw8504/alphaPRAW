# -*- coding: utf-8 -*-
"""
Created on Fri Jun 14 19:14:31 2019

@author: joshl

Functions to scrape Wikipedia and Reddit for Bachelor data
"""

def scrapeWikiEpiTable(seasonNum, show='bachelor'):
    """Scrape Wikipedia for episode table of given season
    
    Uses beautiful soup to scrape the episode table from a given season page of
    The Bachelor or The Bachelorette.
    
    Args:
        seasonNum : An integer variable specifying which season of The
        Bachelor or The Bachelorette to scrape.
        
        show : A string specifying which show to scrape. Set to either
        'bachelor' or 'bachelorette'
        
    Returns:
        A Pandas dataframe of the episode table with the following headers:
            
            ['no.overall', 'no. inseason', 'title', 'original air date',
              'u.s. viewers(millions)', 'description', 'posix time', 
              'show', season']
    
    Raises:
        ValueError : If no or multiple episode tables are found for a given
        season
    """
    import requests
    from bs4 import BeautifulSoup
    import sys
    import numpy as np
    import pandas as pd
    import re
    import datetime as dt
    import time
    
    if show == 'bachelor':
        showStr = 'Bachelor'
    elif show == 'bachelorette':
        showStr = 'Bachelorette'
    else:
        raise ValueError('Show variable must be bachelor or bachelorette.')
    website_url = requests.get(
            'https://en.wikipedia.org/wiki/The_' + 
            showStr + '_(season_' + str(seasonNum) +')'
            ).text
    soup = BeautifulSoup(website_url, features='lxml')
    table_classes = {'class': 'wikiepisodetable'}
    wikitables = soup.findAll("table", table_classes)
    if len(wikitables) == 1:
        table = wikitables[0]
    elif len(wikitables) == 0:
        raise ValueError('I did not find a wikiepisode table!')
    else:
        raise ValueError('I found multiple wikiepisode tables!')
    
    #Parse the column headers and row headers
    colHeads = [h.getText().lower() for h in table.findAll('th', {'scope': 'col'})]
    rowHeads = [h.getText().lower() for h in table.findAll('th', {'scope': 'row'})]
    # pull out on the headers with scope='col', those are the 
    cellList = [c.getText().lower() for c in table.findAll('td')]
    
    #check that row header number * col header number equals cell number
    if len(colHeads) * len(rowHeads) != len(cellList):
        print('Cell number does not match header number')
        sys.exit(1)
    
    #change cell list into numpy array and reshape into pandas dataframe
    cellArray = np.array(cellList).reshape(len(rowHeads), len(colHeads))
    #Have to append rowHead to cellArray since wikiepidsodetables include the
    #row header as part of the row itself
    rowArray = np.array(rowHeads).reshape(len(rowHeads),1)
    finalArray = np.hstack((rowArray, cellArray))
    #add the description entry to column headers
    colHeads.append('description')
    df = pd.DataFrame(finalArray, columns=colHeads)
    
    #Clean the viewership number and convert the original air date to Posix time
    posixTimes = []
    for idx in df.index:
        # remove the reference number in brackets
        df['u.s. viewers(millions)'][idx] = re.sub(
                r" ?\[[^)]+\]", "", df['u.s. viewers(millions)'][idx])
        # find the date in yyyy-mm-dd format
        m = re.search(r'\d\d\d\d-\d\d-\d\d', df['original air date'][idx])
        posixTimes.append(
                int(
                time.mktime(
                dt.datetime.strptime(
                m.group(), '%Y-%m-%d').timetuple())))
    # join posixTimes to df
    df = df.join(pd.DataFrame({'posix time': posixTimes}))
    # join show label to df
    df = df.join(pd.DataFrame({'show' : [show]*df.shape[0]}))
    # join season to df
    df = df.join(pd.DataFrame({'season' : [seasonNum]*df.shape[0]}))
    # set dtype of viewers to float
    df['u.s. viewers(millions)'] = df['u.s. viewers(millions)'].astype('float')
    # set dtype of no. inseason to int
    df['no. inseason'] = df['no. inseason'].astype('int')
    return df[['no.overall', 'no. inseason', 'title', 'original air date', 
              'u.s. viewers(millions)', 'description', 'posix time', 'show',
              'season']]
# keywords to filter in title are 'tell all', 'final rose' and 'finale'
    
def filterData(df):
    """
    Filter out final rose and tell all episodes
    
    Args :
        df : A dataframe generated by scrapeWikiEpiTable.
    
    Returns :
        A dataframe without final rose and tell episode rows
    """
    df = df[~df.title.str.contains('final rose')]
    df = df[~df.title.str.contains('tell all')]
    return df

def appendStartTime(df, deltaDay = 6):
    """
    Parse the posix time of each episode's original airdate append a 'start
    time' column to the dataframe
    
    Args :
        df : A dataframe generated by scrapewikiEpiTable.
        
        deltaDay : An int variable specifying how many days to subtract from 
        original airdate time.
    
    Returns :
        A dataframe with the 'start time' column appended
    """
    import datetime as dt
    
    #parse the posix time column
    times = df.loc[:, 'posix time']
    # loop through times and append to timeList
    timeList = []
    for time in times:
        timeUTC = dt.datetime.utcfromtimestamp(time)
        startTime = timeUTC - dt.timedelta(days=deltaDay)
        timeList.append(int(startTime.timestamp()))
    #append timeList to dataframe as column
    df['startEpoch'] = timeList
    return df

def appendEndTime(df, deltaDay = 0):
    """
    Parse the posix time of each episode's original airdate append a 'start
    time' column to the dataframe
    
    Args :
        df : A dataframe generated by scrapewikiEpiTable.
        
        deltaDay : An int variable specifying how many days to subtract from 
        original airdate time.
    
    Returns :
        A dataframe with the 'start time' column appended
    """
    import datetime as dt
    
    #parse the posix time column
    times = df.loc[:, 'posix time']
    # loop through times and append to timeList
    timeList = []
    for time in times:
        timeUTC = dt.datetime.utcfromtimestamp(time)
        startTime = timeUTC - dt.timedelta(days=deltaDay)
        timeList.append(int(startTime.timestamp()))
    #append timeList to dataframe as column
    df['endEpoch'] = timeList
    return df

def getRedditSubsList(startEpoch, endEpoch, prawUserAgent = 'alpha'):
    """
    Returns a list of Reddit submissions from the bachelor subreddit
    
    Args :
        startEpoch : An int variable of posix time. Inidcates start time of
        reddit submissions.
        
        endEpoch : An int variable of posix time. Indicates end time of reddit
        submissions.
        
        prawUserAgent : The name of the reddit app. This should be specified in
        a praw.ini text file see: 
        https://praw.readthedocs.io/en/latest/getting_started/configuration/prawini.html
    
    Returns :
        subList : A list of reddit submission objects from PRAW
    """
    import praw
    from psaw import PushshiftAPI


    r = praw.Reddit(prawUserAgent)
    api = PushshiftAPI(r)
    subList = list(api.search_submissions(
        before = endEpoch,
        after = startEpoch,
        subreddit = 'thebachelor'))
    return subList

def appendRedditStats(df, prawUserAgent = 'alpha'):
    """
    Appends statistics of submissions titles and comments to provided dataframe
    
    Args :
        df : A dataframe generated by scrapewikiEpiTable.
        
        startEpoch : An int variable of posix time. Inidcates start time of
        reddit submissions.
        
        endEpoch : An int variable of posix time. Indicates end time of reddit
        submissions.
        
        prawUserAgent : The name of the reddit app. This should be specified in
        a praw.ini text file see: 
        https://praw.readthedocs.io/en/latest/getting_started/configuration/prawini.html
    
    Returns :
        df : Original dataframe with columns of reddit statistics appended
    """
    from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
    #import numpy as np

    sia = SIA()
    subLists = []
    titleCntList = []
    titleSentList = []
    bodyCntList = []
    bodySentList = []
    
    for idx, row in df.iterrows():
        subList = getRedditSubsList(
                        row['startEpoch'], row['endEpoch'],
                        prawUserAgent = prawUserAgent)
        subLists.append(subList)
        titleList = []
        bodyList = []
        for sub in subList:
            title = sub.title
            pol_score = sia.polarity_scores(title)
            titleList.append(title)
            titleSentList.append(pol_score['compound'])
            for comment in sub.comments.list():
                body = comment.body
                pol_score = sia.polarity_scores(body)
                bodyList.append(body)
                bodySentList.append(pol_score['compound'])
        titleCntList.append(len(titleList))
        bodyCntList.append(len(bodyList))
    
    df['subNum'] = len(titleCntList)
    df['commentNum'] = len(bodyCntList)
    return df
    